{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1427734",
   "metadata": {},
   "source": [
    "# Regression Models\n",
    "\n",
    "We write a simple model with response $Y$, features $X = (X_1, X_2,...,X_p)$ and error $\\epsilon$ as\n",
    "\n",
    "$$ Y = f(X) + \\epsilon $$\n",
    "\n",
    "The ideal $f(x) = E(Y|X=x)$ where $E(Y|X=x)$ is the expected value of $Y$ at $x$ is known as the <i> regression function</i>.\n",
    "\n",
    "We then say that $f(x)$ is the <i> ideal </i> predictor of $Y$ with regard to the mean-squared prediction error. In other words, $f(x) = E(Y|X=X)$ is the function that minimizes $E[(Y-g(X))^2|X=x]$ over all functions $g$ at all points $X=x$.\n",
    "\n",
    "Our uncontrolled error $\\epsilon = Y - f(x)$ is known as the <i> irreducible error </i>.\n",
    "\n",
    "Thus, For any estimate $\\hat{f}(x)$ of $f(x)$, we have\n",
    "\n",
    "$$ E[(Y - \\hat{f}(X))^2|X=x] = [f(x) - \\hat{f}(x)]^2 + Var(\\epsilon)$$\n",
    "\n",
    "### But how should we estimate $f$?\n",
    "\n",
    "We can relax the definition a let\n",
    "\n",
    "$$ \\hat{f}(x) = \\text{Avg}(Y|X \\in \\mathcal{N}(x)) $$\n",
    "\n",
    "where $\\mathcal{N}$ is some <i> neighborhood </i> of $x$ i.e. a an acceptable range where $x$ could exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd7f7e",
   "metadata": {},
   "source": [
    "# Dimensionality and Structured Models\n",
    "\n",
    "Near neighbor averaging can be pretty good for a small number of features i.e. $p \\leq 4$ and large $N$. When $p$ is large, nearest neighbor methods become less useful due to the <i> curse of dimensionality</i>. Nearest neighbors tend to be far away in high dimensions.\n",
    "\n",
    "We need a reasonable fraction of the $N$ values of $y_i$ to average in order to lower the variance--e.g $10\\%$. In high dimensions, $10\\%$ doesn't need to be local, so we lose the spirit of estimation.\n",
    "\n",
    "## Parametrics and structured models\n",
    "\n",
    "The <i> linear model </i> is the most basic parametric model specified in $p + 1$ parameters\n",
    "\n",
    "$$ f_L(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p$$\n",
    "\n",
    "## Assessing Model Accuracy\n",
    "\n",
    "Suppose we fit a model $\\hat{f}(x)$ to some training data and we wish to see how well it performs. We can compute the average squared predicted error over our training data $(Tr)$:\n",
    "\n",
    "$$ \\text{MSE}_{Tr} = \\text{Avg}_{i \\in Tr} [y_i - \\hat{f}(x_i)]^2 $$\n",
    "\n",
    "This may be biased toward more overfit models. Instead, we should compute our MSE over a test set $(Te)$:\n",
    "\n",
    "$$ \\text{MSE}_{Te} = \\text{Avg}_{i \\in Te} [y_i - \\hat{f}(x_i)]^2 $$\n",
    "\n",
    "We want to minimize the mean-squared error across the training data $Tr$ and test data $Te$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52273faf-dfdb-4f9a-8443-ca0b8419e345",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff\n",
    "\n",
    "If we fit a model $\\hat{f}(x)$ to a set of training data $Tr$ and the true model is $Y = f(X) + \\epsilon \\rightarrow f(x) = E(Y|X=x)$, then \n",
    "\n",
    "$$ \\text{E}(y_0 - \\hat{f}(x_0))^2 = \\text{Var}(\\hat{f}(x_0)) + [\\text{Bias}(\\hat{f}(x_0))]^2 + \\text{Var}(\\epsilon) $$\n",
    "\n",
    "where $$ \\text{Bias}(\\hat{f}(x_0)) = \\text{E}(\\hat{f}(x_0)) - f(x_0) $$\n",
    "\n",
    "In other words, the bias is the difference between the average prediction of $x_0$ average over the training data sets and the truth of $f(x_0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c03ecc-321d-4470-8d63-5b4c789c7d50",
   "metadata": {},
   "source": [
    "# Classification Problems\n",
    "\n",
    "In a classification problem, our response $Y$ is qualitative as opposed to quantitative.\n",
    "\n",
    "$$ Y = \\mathcal{C}(X) $$\n",
    "\n",
    "Suppose there are $K$ elements in $\\mathcal{C}$ numbered $1,2,...,K$. Let the <i>conditional class probability</i> at $x$ be\n",
    "\n",
    "$$ p_k(x) = P(Y=k|X=x) \\; \\; \\; k=1,2,...,K $$\n",
    "\n",
    "The <i>Bayes optimal classifier</i> at $x$ is \n",
    "\n",
    "$$ \\mathcal{C}(x) = j \\; \\; \\; \\text{if} \\; \\; \\; P_j(x) = \\text{max}\\{p_1(x),p_2(x),...,p_k(x)\\} $$\n",
    "\n",
    "We can measure th performance of $\\hat{\\mathcal{C}}(x)$ using the misclassification error rate:\n",
    "\n",
    "$$ \\text{Err}_{Te} = \\text{Avg}_{i \\in Te} \\text{I}[y_i \\neq \\hat{\\mathcal{C}}(x_i)] $$\n",
    "\n",
    "The Bayes classifier has the smallest error in the population.  Structed models for $\\mathcal{C}(x)$ include support vector machines, logistic regression, generalized additive models, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be352d86-8f7c-4014-887a-a7ef649f7c71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
